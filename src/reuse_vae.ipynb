{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========public pkgs========\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#========= private pkgs==========================\n",
    "from load_data import custom_datasets\n",
    "from load_data import custom_transform\n",
    "\n",
    "from load_data_h5py import SDFDataset\n",
    "from SDF_VAE_improved import Encoder,Decoder,SDF_VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_symmetric(a_numpy):\n",
    "    \"\"\"\n",
    "    Mirror an N x N x N SDF matrix to create a symmetric 2N x 2N x 2N SDF matrix.\n",
    "\n",
    "    :param a_numpy: An N x N x N numpy array representing the SDF in positive XYZ space.\n",
    "    :return: A 2N x 2N x 2N symmetric numpy array representing the full SDF.\n",
    "    \"\"\"\n",
    "    N = a_numpy.shape[0]\n",
    "    \n",
    "    a_numpy = np.flip(a_numpy, axis=2)\n",
    "\n",
    "\n",
    "    a_numpy = np.flip(a_numpy, axis=1)\n",
    "\n",
    "\n",
    "    a_numpy = np.flip(a_numpy, axis=0)\n",
    "\n",
    "    # Initialize the full symmetric matrix of size 2N x 2N x 2N\n",
    "    full_matrix = np.zeros((2 * N, 2 * N, 2 * N))\n",
    "\n",
    "    # Populate the full matrix by mirroring a_numpy across its dimensions\n",
    "    # Front-top-left quarter\n",
    "    full_matrix[:N, :N, :N] = a_numpy\n",
    "    # Front-top-right quarter\n",
    "    full_matrix[:N, N:, :N] = a_numpy[:, ::-1, :]\n",
    "    # Front-bottom-left quarter\n",
    "    full_matrix[N:, :N, :N] = a_numpy[::-1, :, :]\n",
    "    # Front-bottom-right quarter\n",
    "    full_matrix[N:, N:, :N] = a_numpy[::-1, ::-1, :]\n",
    "\n",
    "    # Back mirrors of the front\n",
    "    full_matrix[:, :, N:] = full_matrix[:, :, :N][:, :, ::-1]\n",
    "\n",
    "    # return full_matrix\n",
    "    return a_numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================Load configuration file===============\n",
    "with open('./config_cluster.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Extract configuration parameters\n",
    "batch_size = config['model_params']['batch_size']\n",
    "latent_dim = config['model_params']['latent_dim']\n",
    "beta = config['model_params']['beta']\n",
    "learning_rate = config['train_params']['learning_rate']\n",
    "epochs = config['train_params']['epochs']\n",
    "manual_seed = config['random_seed']['manual_seed']\n",
    "cuda_manual_seed = config['random_seed']['cuda_manual_seed']\n",
    "loading_checkpoint = config['train_params']['loading_checkpoint']\n",
    "# Paths from configuration\n",
    "data_path_train = config['Path']['train_data_path']\n",
    "data_path_test = config['Path']['test_data_path']\n",
    "save_path = config['Path']['save_path']\n",
    "checkpoint_path = config['Path']['log_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.cuda.manual_seed(cuda_manual_seed)\n",
    "sdf_dimen = 50\n",
    "# load test and train data\n",
    "dataset_train = SDFDataset(data_path_train)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_train = SDFDataset(data_path_test)\n",
    "loader_test = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "model = SDF_VAE(input_channels=1, latent_dim=latent_dim, D=sdf_dimen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is only available\n"
     ]
    }
   ],
   "source": [
    "# Setup device (GPU/CPU)\n",
    "if torch.cuda.is_available(): # GPU is available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print('GPU is available')\n",
    "        model = nn.DataParallel(model)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    GPU = True\n",
    "    model.to(device)\n",
    "else:  # only cpu is available\n",
    "    print('CPU is only available')\n",
    "    GPU = False\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "# Load checkpoint if specified\n",
    "if loading_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_path+'checkpoint.tar')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "else: # use the initial model and optimiser\n",
    "    current_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def lossfunc(sdf,sdf_hat,iso,iso_hat,mu,logvar,beta):\n",
    "    \"\"\"\n",
    "    Computes the Variational Autoencoder (VAE) loss function, combining reconstruction loss and KL divergence.\n",
    "    Args\n",
    "    Returns:\n",
    "        torch.Tensor: The computed loss value.\n",
    "    \"\"\"\n",
    "    # print('sdf_hat',sdf_hat.shape)\n",
    "    # print('sdf',sdf.shape)\n",
    "    # print('iso_hat',iso_hat.shape)\n",
    "    # print('iso',iso.shape)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    sdf_loss = F.mse_loss(sdf_hat, sdf,reduction = 'mean')\n",
    "    iso_loss = F.mse_loss(iso_hat, iso,reduction = 'mean')\n",
    "    recons_loss = sdf_loss+ iso_loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # kl_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim = 1), dim = 0)\n",
    "    return recons_loss + beta* kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reuse the model on cpu machine, \n",
    "### the trained model from GPU or parallel gpus, they are different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SDF_VAE(\n",
       "  (encoder): Encoder(\n",
       "    (decoder_sdf): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv3d(1, 16, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv3d(16, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv3d(32, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv3d(32, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (decoder_iso): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=160, out_features=6, bias=True)\n",
       "    (fc_logvar): Linear(in_features=160, out_features=6, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_sdf_first_layer): Linear(in_features=4, out_features=512, bias=True)\n",
       "    (decoder_sdf): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(3, 3, 3), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Sequential(\n",
       "      (0): ConvTranspose3d(32, 32, kernel_size=(41, 41, 41), stride=(3, 3, 3), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Conv3d(32, 1, kernel_size=(2, 2, 2), stride=(1, 1, 1))\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "    (decoder_iso): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=20, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=20, out_features=8, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "mode_index = '192'\n",
    "PATH  = './save_model/model_for_resol_50/'\n",
    "loaded_model = PATH+'/VAEmodel_'+mode_index+'.pt'\n",
    "def load_model(model, path):\n",
    "    # Load the state dictionary from the file.\n",
    "    state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # Check if the model was trained and saved using DataParallel or DistributedDataParallel\n",
    "    # by checking the presence of 'module.' prefix in the state dictionary keys.\n",
    "    is_multi_gpu_model = any(k.startswith('module.') for k in state_dict.keys())\n",
    "\n",
    "    if is_multi_gpu_model:\n",
    "        # If the model was saved with 'module.' prefixes (indicative of DataParallel or DistributedDataParallel usage),\n",
    "        # create a new state dictionary without these prefixes.\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            # Remove the 'module.' prefix.\n",
    "            name = k[7:] if k.startswith('module.') else k\n",
    "            new_state_dict[name] = v\n",
    "        state_dict = new_state_dict\n",
    "\n",
    "    # If there are no 'module.' prefixes, the state_dict is assumed to be from a single GPU training\n",
    "    # and is used without modification.\n",
    "\n",
    "    # Load the state dictionary into the model.\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "model = load_model(model, PATH+'/VAEmodel_'+mode_index+'.pt')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.load('./save_model//model_for_resol_50/mu_list_'+str(mode_index)+'.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decodeed sdf torch.Size([1, 1, 50, 50, 50])\n",
      "decodeed iso torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# sdf_z,iso_z = model.decoder(mu)\n",
    "index = 52\n",
    "sdf_z,iso_z = model.decoder(mu[index,:].unsqueeze(0))\n",
    "\n",
    "import numpy as np\n",
    "sdf = make_symmetric(sdf_z.squeeze().detach().numpy())\n",
    "\n",
    "iso = iso_z.detach().numpy()\n",
    "path_recon = './data/reconstruct_sdf/'\n",
    "np.save(path_recon+'sdf'+str(index)+'.npy',sdf)\n",
    "np.save(path_recon+'iso'+str(index)+'.npy',iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_1 = mu[10,:].unsqueeze(0)\n",
    "mu_2 = mu[80,:].unsqueeze(0)\n",
    "\n",
    "# mu_1 = mu[45,:].unsqueeze(0)\n",
    "# mu_2 = mu[88,:].unsqueeze(0)\n",
    "def interpolate_betweeen_A_and_B(latent_A,latent_B,N):\n",
    "    ''' inteprolatte of two latent vector 'A' 'B' ,with N stepts\n",
    "    '''\n",
    "    code = torch.Tensor(N, 6).to(device)\n",
    "    for i in range(N):\n",
    "        code[i] = i / (N - 1) * latent_A + (1 - i / (N - 1) ) * latent_B\n",
    "    return code\n",
    "\n",
    "Nbcell = 50\n",
    "mu_inter = interpolate_betweeen_A_and_B(mu_1, mu_2, Nbcell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decodeed sdf torch.Size([100, 1, 50, 50, 50])\n",
      "decodeed iso torch.Size([100])\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n",
      "(50, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "sdf_z,iso_z = model.decoder(mu_inter)\n",
    "for ii in range(Nbcell):\n",
    "    sdf = sdf_z[ii].squeeze().detach().cpu().numpy()\n",
    "    print(sdf.shape)\n",
    "    iso = iso_z[ii].detach().cpu().numpy()\n",
    "    np.save(path_recon+'sdf'+str(ii),sdf)\n",
    "    np.save(path_recon+'iso'+str(ii), iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 50, 50, 50])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_z.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
